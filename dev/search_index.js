var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Core-Types","page":"API Reference","title":"Core Types","text":"","category":"section"},{"location":"api/#WeightedSampling.SMCKernel","page":"API Reference","title":"WeightedSampling.SMCKernel","text":"SMCKernel{S,L,W}\n\nRepresents a (random weight) importance sampler.\n\nFields\n\nsampler::S: Function (args...) -> sample that generates samples\nweighter::W: Function (args..., sample) -> log_weight that computes log weights. Can be random. Use nothing for uniform weights.\nlogpdf::L: Function (args..., sample) -> log_density that evaluates log-density\n\nEvery SMCKernel represents a stochastic kernel given by averaging samples over weights:\n\nint_w textweighter(w mid textargs x) textsampler(x mid textargs) dw\n\nThe logpdf function is the density of this kernel.\n\nConstructor\n\nSMCKernel(sampler, weighter, logpdf)\n\nExamples\n\nusing Distributions\n\n# Custom truncated normal kernel\nTruncatedNormal = SMCKernel(\n    (μ, σ, a, b) -> rand(Truncated(Normal(μ, σ), a, b)),\n    nothing,  # uniform weights\n    (μ, σ, a, b, x) -> logpdf(Truncated(Normal(μ, σ), a, b), x)\n)\n\n# Use in @smc model\n@smc function model(data)\n    θ ~ TruncatedNormal(0.0, 1.0, -2.0, 2.0)\n    for y in data\n        y => Normal(θ, 0.5)\n    end\nend\n\nSee also: @smc\n\n\n\n\n\n","category":"type"},{"location":"api/#The-@smc-macro","page":"API Reference","title":"The @smc macro","text":"","category":"section"},{"location":"api/#WeightedSampling.@smc","page":"API Reference","title":"WeightedSampling.@smc","text":"@smc function model(args...)\n    # probabilistic program\nend\n\nTransform a Julia function into an SMC sampler.\n\nGenerated Functions\n\nThe macro generates two functions:\n\nmodel(args...; n_particles=1000, ...): Creates new particles and returns (particles, evidence)\nmodel!(args...; particles, ...): Updates existing particles in-place and returns evidence\n\nwhere\n\nargs...: Model-specific arguments (data, hyperparameters, etc.)\nn_particles::Int=1000: Number of particles to create\nparticles::DataFrame: Existing particle DataFrame to update in-place\n\nBoth functions accept the following keyword arguments:\n\ness_perc_min::Float64=0.5: Minimum effective sample size percentage for resampling trigger\nkernels::NamedTuple=nothing: Custom SMC kernels (merged with defaults)\nproposals::NamedTuple=nothing: Custom proposal kernels for MH moves (merged with defaults)\ncompute_evidence::Bool=true: Whether to compute evidence (log marginal likelihood)\nshow_progress::Bool=true: Whether to show progress bar\n\nOperators\n\nWithin @smc, the following operators are available:\n\nParticle assignment: x .= expr broadcasts expr to particles.x.\nSampling: x ~ SMCKernel(args) samples to particles.x and updates weights.\nObservation: expr => SMCKernel(args) updates weights based on observing expr.\nMH Move: x << Proposal(args) performs Metropolis-Hastings moves on particles.x.\n\nBoth expr and args can reference particles.x as x.\n\nRegular Julia constructs are supported:\n\nAssignment =: Creates local variables (not stored in particles).\nFor loops: Supports for i in collection and tuple destructuring, where collection does not involve particle variables.\nConditionals: if condition where condition does not involve particle variables.\n\nAdditional features:\n\nArray indexing: x[i] maps over particles.x.\nIndex interpolation: x{i} creates dynamic variable names (e.g., x1, x2, ...).\n\nExamples\n\n# Linear regression with moves\n@smc function linear_regression(xs, ys)\n    α ~ Normal(0, 10)\n    β ~ Normal(0, 10)\n    for (x, y) in zip(xs, ys)\n        y => Normal(α + β * x, 1.0)\n        if resampled\n            (α, β) << autoRW()\n        end\n    end\nend\n\nxs = 1:10\nys = 1-0 .- 0.5 .* xs .+ 0.5 * randn(length(xs))\n\nparticles, evidence = linear_regression(xs, ys, n_particles=1000, ess_perc_min=0.5)\ndescribe_particles(particles)\n\nSee also: SMCKernel\n\n\n\n\n\n","category":"macro"},{"location":"api/#Utility","page":"API Reference","title":"Utility","text":"","category":"section"},{"location":"api/#WeightedSampling.@E","page":"API Reference","title":"WeightedSampling.@E","text":"@E(f, particles)\n\nCompute the weighted expectation of an anonymous function f with respect to particle samples.\n\nArguments\n\nf: Anonymous function where argument names reference particle column names\nparticles: DataFrame with particle samples and :weights column\n\nReturns\n\nWeighted expectation mathbbEf = sum_i=1^N w_i f(x p_i), where w_i are normalized particle weights.\n\nExamples\n\n# Expectation of `particles.x`\nposterior_mean = @E(x -> x, particles)\n\n# Expectation of `sin.(particles.x) + cos.(particles.y)`\nexpectation = @E((x,y) -> sin(x) + cos(y), particles)\n\n\n\n\n\n","category":"macro"},{"location":"api/#WeightedSampling.exp_norm","page":"API Reference","title":"WeightedSampling.exp_norm","text":"exp_norm(weights::Vector{Float64})\n\nExponentiate and normalize log weights.\n\nArguments\n\nweights::Vector{Float64}: Vector of log weights.\n\nReturns\n\nVector of normalized probabilities.\n\nExamples\n\nexp_norm(particles.weights)\n\n\n\n\n\n","category":"function"},{"location":"api/#WeightedSampling.describe_particles","page":"API Reference","title":"WeightedSampling.describe_particles","text":"describe_particles(particles::DataFrame; cols=nothing)\n\nCompute weighted descriptive statistics for particle variables.\n\nArguments\n\nparticles::DataFrame: DataFrame with particle samples and :weights column containing log weights\ncols=nothing: Vector of column names to analyze (default: all numeric columns except :weights)\n\nReturns\n\nDataFrame with columns:\n\nvariable::Symbol: Variable name  \nmean: Weighted mean (scalar or vector)\nmedian: Weighted median (scalar or vector)\nstd: Weighted standard deviation (scalar or vector)\nmin: Minimum value (scalar or vector)\nmax: Maximum value (scalar or vector) \ness::Float64: Effective sample size textESS = frac1sum_i=1^N w_i^2\n\nExamples\n\ndescribe_particles(particles)\n\nThe function handles both scalar and vector-valued particle variables. For vector variables, statistics are computed component-wise and returned as vectors.\n\n\n\n\n\n","category":"function"},{"location":"api/#WeightedSampling.sample_particles","page":"API Reference","title":"WeightedSampling.sample_particles","text":"sample_particles(particles::DataFrame, n::Int; replace::Bool=true)\n\nRandomly sample n rows from a DataFrame of weighted particles according to their log weights.\n\nArguments\n\nparticles::DataFrame: DataFrame with particle samples and :weights column containing log weights\nn::Int: Number of samples to draw\nreplace::Bool=true: Whether to sample with replacement\n\nReturns\n\nDataFrame with n sampled particles.\n\nExamples\n\nposterior_samples = sample_particles(particles, 1000)\n\n\n\n\n\n","category":"function"},{"location":"api/#WeightedSampling.diversity","page":"API Reference","title":"WeightedSampling.diversity","text":"diversity(particles, targets)\n\nCompute particle diversity as the fraction of unique particle combinations.\n\nArguments\n\nparticles::DataFrame: DataFrame with particle samples\ntargets: Vector of column names to consider for diversity calculation\n\nReturns\n\nFloat64 between 0 and 1, where:\n\n1.0 = all particles in subset are unique\n0.0 = all particles in subset are identical\n\nExamples\n\n# Conditional move based on diversity\nif diversity(particles, [:x]) < 0.5\n    x << autoRW()\nend\n\n\n\n\n\n","category":"function"},{"location":"api/#MH-proposal-kernels","page":"API Reference","title":"MH proposal kernels","text":"","category":"section"},{"location":"api/#WeightedSampling.autoRW","page":"API Reference","title":"WeightedSampling.autoRW","text":"autoRW(particles, targets, min_step=1e-3)\n\nAdaptive random walk proposal kernel with empirically calibrated covariance.\n\nArguments\n\nparticles::DataFrame: Current particle set  \ntargets::Vector{Symbol}: Variable names to update\nmin_step=1e-3: Minimum step size to prevent singular covariance\n\nReturns\n\nproposals::DataFrame: Proposed new values for target variables\nlog_ratios::Vector{Float64}: Log proposal ratios (zero since proposal is symmetric)\n\nDescription\n\nPerforms adaptive random walk using the empirical covariance of target particles:\n\nx^textnew = x^textold + epsilon quad epsilon sim mathcalN(0 lambda Sigma)\n\nwhere:\n\nlambda = 238 d^(-12) is the optimal scaling factor for d-dimensional problems\nSigma is the weighted empirical covariance matrix of the target particles\nCovariance elements are replaced with min_step if they are zero\n\nExamples\n\n@smc function model()\n    α ~ Normal(0, 10)\n    β ~ Normal(0, 10)\n    \n    α << autoRW()\n    (α, β) << autoRW()\nend\n\nSee also: @smc\n\n\n\n\n\n","category":"function"},{"location":"api/#WeightedSampling.RW","page":"API Reference","title":"WeightedSampling.RW","text":"RW(particles, targets, step_size)\n\nSymmetric random walk proposal kernel with variance step_size.\n\nArguments\n\nparticles::DataFrame: Current particle set\ntargets::Vector{Symbol}: Variable names to update\nstep_size: Step size for random walk\n\nReturns\n\nproposals::DataFrame: Proposed new values for target variables\nlog_ratios::Vector{Float64}: Log proposal ratios (zero since proposal is symmetric)\n\nExamples\n\n@smc function model()\n    α ~ Normal(0, 10)\n    β ~ Normal(0, 10)\n    \n    α << RW(0.1)\n    (α, β) << RW(0.1)\nend\n\nSee also: @smc\n\n\n\n\n\n","category":"function"},{"location":"api/#Distribution-kernels","page":"API Reference","title":"Distribution kernels","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"WeightedSampling.jl provides kernels for all major distributions from Distributions.jl. These can be used directly by name in @smc functions:","category":"page"},{"location":"api/#Continuous-distributions","page":"API Reference","title":"Continuous distributions","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Normal(μ, σ), Beta(α, β), Gamma(α, θ), Exponential(θ), Uniform(a, b), Cauchy(μ, σ), Laplace(α, θ), LogNormal(μ, σ), Weibull(α, θ), Chi(ν), Chisq(ν), TDist(ν), FDist(ν1, ν2), Pareto(α, θ), Rayleigh(σ), Gumbel(μ, θ), Frechet(α, θ), InverseGamma(α, θ), LogitNormal(μ, σ), Logistic(μ, θ), SkewNormal(ξ, ω, α), SkewedExponentialPower(μ, σ, p, α), VonMises(μ, κ), GeneralizedPareto(μ, σ, ξ), NoncentralChisq(ν, λ), NoncentralF(ν1, ν2, λ), NoncentralT(ν, λ), NormalCanon(η, λ)","category":"page"},{"location":"api/#Discrete-distributions","page":"API Reference","title":"Discrete distributions","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Bernoulli(p), BernoulliLogit(logitp), Binomial(n, p), BetaBinomial(n, α, β), Categorical(p), DiscreteUniform(a, b), Geometric(p), Hypergeometric(s, f, n), NegativeBinomial(r, p), Poisson(λ), PoissonBinomial(p), DiscreteNonParametric(xs, ps), Dirac(x)","category":"page"},{"location":"api/#Multivariate-distributions","page":"API Reference","title":"Multivariate distributions","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"MvNormal(μ, Σ), MvNormalCanon(h, J), MvLogNormal(μ, Σ), MvLogitNormal(μ, Σ), Multinomial(n, p), Dirichlet(α), LKJ(d, η), LKJCholesky(d, η), Wishart(ν, S), InverseWishart(ν, Ψ)","category":"page"},{"location":"api/#Matrix-valued-distributions","page":"API Reference","title":"Matrix-valued distributions","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"MatrixNormal(M, U, V), MatrixBeta(p, n1, n2), MatrixFDist(n1, n2, B), MatrixTDist(ν, M, Σ, Ω)","category":"page"},{"location":"api/#Advanced-usage","page":"API Reference","title":"Advanced usage","text":"","category":"section"},{"location":"api/#Custom-kernels","page":"API Reference","title":"Custom kernels","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"You can define custom SMC kernels for distributions not included in the package:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"using Distributions\n\n# Custom truncated normal kernel\nTruncatedNormal = SMCKernel(\n    (μ, σ, a, b) -> rand(Truncated(Normal(μ, σ), a, b)),\n    nothing,  # uniform weights\n    (μ, σ, a, b, x) -> logpdf(Truncated(Normal(μ, σ), a, b), x)\n)\n\n@smc function model()\n    θ ~ TruncatedNormal(0.0, 1.0, -2.0, 2.0)\nend\n\n# Pass custom kernel to the model\nparticles, evidence = model(\n    kernels=(TruncatedNormal=TruncatedNormal,)\n)","category":"page"},{"location":"api/#Custom-proposals","page":"API Reference","title":"Custom proposals","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"You can defined custom MH proposals using the following signature:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"function Proposal(particles, targets, params...)\n    # Return (proposed_changes::DataFrame, log_ratios::Vector)\nend","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Here log_ratios is a vector with entries log q(x_textold mid x_textnew) - log q(x_textnew mid x_textold), where q is the density of the proposal kernel.","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"WeightedSampling.jl provides the @smc macro for concise specification of Sequential Monte Carlo (SMC) sampling schemes. It transforms probabilistic programs into efficient particle filters with automatic resampling and weight management. It excels at dynamic and moderate-dimensional statistical models (< 1k parameters).","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Note: This package is in early development. Use with caution.","category":"page"},{"location":"usage_guide/#Installation","page":"Usage Guide","title":"Installation","text":"","category":"section"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"The package is not yet registered in the Julia General Registry. Install directly from this repository:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"add https://github.com/MariusFurter/WeightedSampling.jl","category":"page"},{"location":"usage_guide/#Quick-start","page":"Usage Guide","title":"Quick start","text":"","category":"section"},{"location":"usage_guide/#Linear-regression","page":"Usage Guide","title":"Linear regression","text":"","category":"section"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"using WeightedSampling\n\n@smc function linear_regression(xs, ys)\n    α ~ Normal(0, 1) # sample\n    β ~ Normal(0, 1)\n    for (x, y) in zip(xs, ys)\n        y => Normal(α + β * x, 1.0) # observe\n    end\nend\n\nxs = 1:10\nys = 1-0 .- 0.5 .* xs .+ 0.5 * randn(length(xs))\n\nparticles, evidence = linear_regression(xs, ys, n_particles=1000, ess_perc_min=0.5)\ndescribe_particles(particles)","category":"page"},{"location":"usage_guide/#Bootstrap-particle-filter","page":"Usage Guide","title":"Bootstrap particle filter","text":"","category":"section"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"@smc function ssm(observations)\n    I = [1 0; 0 1]\n    x .= [0.0, 0.0]\n    v .= [1.0, 0.0]\n    for obs in observations\n        x .= x + v                   # update position\n        dv ~ MvNormal([0,0], 0.1*I)  # sample process noise\n        v .= v + dv                  # update velocity\n        obs => MvNormal(x, 0.5*I)    # observe\n    end\nend","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"The @smc macro generates functions that propagate a DataFrame particles of weighted samples through SMC kernels. Particle variables are stored as columns (e.g., particles.x), with log-weights in particles.weights. Resampling occurs when the effective sample size drops below ess_perc_min. The macro generates two SMC functions:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"# In-place (modifies existing particles)\nfunction model!(args...; particles, kernels=nothing, proposals=nothing, \n                ess_perc_min=0.5, compute_evidence=true, show_progress=true)\n\n# Sampling (creates new particles)\nfunction model(args...; n_particles=1000, kernels=nothing, proposals=nothing,\n               ess_perc_min=0.5, compute_evidence=true, show_progress=true)","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Pass external data as function arguments. Both functions return the evidence (log-probability of observations) by default.","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Within @smc, the following operators are available:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Particle assignment: x .= expr broadcasts expr to particles.x.\nSampling: x ~ SMCKernel(args) samples to particles.x and updates weights.\nObservation: expr => SMCKernel(args) updates weights based on observing expr.\nMH Move: x << Proposal(args) performs Metropolis-Hastings moves on particles.x.","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Both expr and args can reference particles.x as x.","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Regular Julia constructs are supported:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Assignment (=): Creates local variables (not stored in particles).\nFor loops: Supports for i in collection and tuple destructuring, where collection does not involve particle variables.\nConditionals: if condition where condition does not involve particle variables.","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Additional features:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Array indexing: x[i] maps over particles.x.\nIndex interpolation: x{i} creates dynamic variable names (e.g., x1, x2, ...).","category":"page"},{"location":"usage_guide/#SMCKernel-type","page":"Usage Guide","title":"SMCKernel type","text":"","category":"section"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"SMCKernel represents a (random weight) importance sampler:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"sampler: (args...) -> sample — generates samples\nweighter: (args..., sample) -> log_weight — computes log weights (can be nothing for uniform)\nlogpdf: (args..., sample) -> log_pdf — evaluates log-density","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Every SMCKernel represents a stochastic kernel given by averaging samples over weights:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"int_w textweighter(w mid textargs x) textsampler(x mid textargs) dw","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"The logpdf is the log-density of this kernel.","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Default kernels are provided for major distributions from Distributions.jl (see smc_kernels.jl), accessible by name in @smc. Custom kernels can be defined using SMCKernel and passed as named tuples to SMC functions.","category":"page"},{"location":"usage_guide/#MH-moves","page":"Usage Guide","title":"MH moves","text":"","category":"section"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Resampling reduces particle diversity for early-sampled variables. MH moves can restore diversity. Example:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"@smc function linear_regression(xs, ys)\n    α ~ Normal(0, 10)\n    β ~ Normal(0, 10)\n    for (x, y) in zip(xs, ys)\n        y => Normal(α + β * x, 1.0)\n        if resampled\n            α << autoRW()\n            β << autoRW()\n        end\n    end\nend","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Note: Moves require that particle variables are not overwritten before the move, as the MH acceptance ratio depends on previous values.","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Moves are computationally expensive. Use them conditionally, based on the state of the particle approximation. The following variables are available within @smc:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"particles: The particles DataFrame\nresampled: Boolean, true if resampling occurred in the previous step\ness_perc: Current effective sample size (percent)\nevidence: Current accumulated log-probability","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"MH kernels are functions Proposal(particles, targets, args...) returning a DataFrame of proposals and a vector of log proposal ratios.","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Available proposals:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"RW(step_size): Symmetric random walk\nautoRW(min_step): Random walk with empirically calibrated covariance","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Joint updates are supported:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"(α, β) << autoRW()","category":"page"},{"location":"usage_guide/#Utility-functions","page":"Usage Guide","title":"Utility functions","text":"","category":"section"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"sample_particles(particles, n; replace=false): Draw n samples by weight\ndescribe_particles(particles): Summarize all variables\nexp_norm(weights): Normalize log-weights to probabilities\n@E(f, particles): Compute weighted expectation of function f over particle variables","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Examples:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"@E(x -> x, particles)         # E[x]\n@E(x -> x == 1, particles)    # P[x == 1]\n@E((x, y) -> x + y, particles) # E[x + y]","category":"page"},{"location":"usage_guide/#Performance-tips","page":"Usage Guide","title":"Performance tips","text":"","category":"section"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Main bottleneck is resampling; many distinct variables (>10k) slow performance.\nOverwriting particle variables marginalizes them.\nUse static, concrete types (e.g., StaticArrays.SVector) for efficiency.\nMoves are expensive; use only when necessary.","category":"page"},{"location":"usage_guide/#Saving-particle-history","page":"Usage Guide","title":"Saving particle history","text":"","category":"section"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"Monitor SMC progress by saving snapshots:","category":"page"},{"location":"usage_guide/","page":"Usage Guide","title":"Usage Guide","text":"ess_list = []\nhistory = []\n\n@smc function ssm(observations, ess_list, history)\n    I = [1 0; 0 1]\n    x .= [0.0, 0.0]\n    v .= [1.0, 0.0]\n\n    for obs in observations\n        # Save snapshot\n        push!(ess_list, ess_perc)\n        push!(history, particles.x)\n\n        x .= x + v\n        dv ~ MvNormal([0,0], 0.1*I)\n        v .= v + dv\n        obs => MvNormal(x, 0.5*I)\n    end\nend","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"For complete runnable examples with plotting, see the examples/ directory in the repository.","category":"page"},{"location":"#WeightedSampling.jl","page":"Home","title":"WeightedSampling.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Build Status)","category":"page"},{"location":"","page":"Home","title":"Home","text":"WeightedSampling.jl provides a macro-based interface for Sequential Monte Carlo (SMC) in Julia. It enables concise, readable probabilistic programs that are transformed into efficient particle filters with resampling and weight management.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Intuitive @smc macro for SMC model specification\nAutomatic weight management and resampling\nSupport for Metropolis-Hastings moves within SMC\nFlexible kernel and proposal definitions\nUtility functions for particle analysis and sampling","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is not yet registered. Install directly from GitHub:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"https://github.com/MariusFurter/WeightedSampling.jl\")","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here's a simple linear regression example:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using WeightedSampling\n\n@smc function linear_regression(xs, ys)\n    α ~ Normal(0, 1)  # sample prior\n    β ~ Normal(0, 1)\n    \n    for (x, y) in zip(xs, ys)\n        y => Normal(α + β * x, 1.0)  # observe data\n    end\nend\n\n# Generate some data\nxs = 1:10\nys = 2.0 .+ 0.5 .* xs .+ randn(length(xs))\n\n# Run SMC inference\nparticles, evidence = linear_regression(xs, ys, n_particles=1000)\n\n# Analyze results\ndescribe_particles(particles)","category":"page"},{"location":"#The-@smc-Macro","page":"Home","title":"The @smc Macro","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The core of WeightedSampling.jl is the @smc macro, which transforms a Julia function into a Sequential Monte Carlo (SMC) model with automatic weight management, resampling, and support for Metropolis-Hastings (MH) moves.","category":"page"},{"location":"#Key-Operators","page":"Home","title":"Key Operators","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Particle assignment: x .= expr broadcasts expr to all particles\nSampling: x ~ SMCKernel(args) samples from a distribution or kernel and updates particle weights\nObservation: expr => SMCKernel(args) conditions on observed data, updating weights accordingly\nMH Move: x << Proposal(args) applies MH moves to particles","category":"page"},{"location":"#Generated-Functions","page":"Home","title":"Generated Functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SMC models defined with @smc are compiled to two main functions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"model!(args...; particles, ...) (in-place update of existing particles)\nmodel(args...; n_particles=..., ...) (creates and returns new particles)","category":"page"},{"location":"#Navigation","page":"Home","title":"Navigation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Usage Guide: Detailed usage instructions and examples\nAPI Reference: Complete API documentation\nExamples: Additional examples and use cases","category":"page"},{"location":"#Note","page":"Home","title":"Note","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is in early development. Feedback is welcome!","category":"page"}]
}
